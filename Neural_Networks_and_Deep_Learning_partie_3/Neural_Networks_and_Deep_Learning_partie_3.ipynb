{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cours n° 1 : semaine 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois, on aborde le cas des **réseaux de neurones**<br>\n",
    "On va avoir cette fois une couche d’entrée correspondant aux données en entrée, plusieurs couches cachées et une couche de sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la régression logistique, seule la fonction $sigmoïde$ était utilisée puisqu’elle représente dans ce cas une probabilité appartenant à $[0,1]$.<br>\n",
    "Pour le cas des réseaux de neurones, pour chacune des couches cachées, on peut utiliser d’autres fonctions d’activations telles que « $tanh$ ». Mais un défaut pour  la fonction \"$sigmoïde$\" et la fonction « $tanh$ » réside dans le fait qu’elles possèdent des dérivées proches de zéro pour des valeurs très grandes ou très faibles, ce qui a pour effet de ralentir fortement l’algorithme « gradient descent ».<br>\n",
    "C’est pourquoi il vaut mieux utiliser plutôt la fonction **« Relu » ** (**Re**ctified **L**inear **U**nit) qui est très utilisée mais on utilise aussi « $tanh$ » pour les couches cachées des réseaux de neurones.\n",
    "Néanmoins la fonction « $Relu$ » possède comme inconvénient le fait que sa dérivée est nulle si $z < 0$ d’où la possibilité d’utiliser plutôt la fonction « $Leaky Relu$ » qui possède une pente  positive pour les $z < 0$. <br>\n",
    "Mais en fait « $Leaky Relu$ » n’est pas beaucoup utilisée en pratique. Et donc si il faut choisir, on peut utiliser simplement la fonction « $Relu$ »."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction « $Relu$ » va permettre d’avoir un apprentissage plus rapide pour l’algorithme « gradient descent » car la pente de cette fonction est toujours très différente de zéro.<br>\n",
    "De plus, en pratique les valeurs de $z$ sont la plupart du temps, positives ce qui rend l’utilisation de la fonction « $Relu$» très efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantages et défaut des différentes fonctions d’activation :\n",
    "\n",
    "- fonction $sigmoïde$ : à utiliser seulement pour la couche de sortie dans le cas de « classification binaire » et jamais pour les couches cachées\n",
    "- fonction « $tanh$ » : cette fonction est beacoup plus adaptée que la fonction sigmoïde pour les couches cachées est « tanh » \n",
    "- fonction « $Relu$ » : est la fonction qui est la plus communément utilisée pour les couches cachées. Avec : $a =  max(0,z)$\n",
    "- fonction « $Leaky Relu$ » : moins utilisée en pratique. Avec : $a =  max(0.01*z,z)$ (On pourrait utiliser une constante autre que $0.01$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre remarque : ces fonctions \"$tanh$\",\"$Relu$\" et \"$LeakyRelu$\" permettent d’obtenir des activations « $a$ » centrées en zéro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions d’activations de type non linéaire sont utiles, car si on employait uniquement des fonctions linéaires, cela reviendrait simplement à réaliser une simple régression logistique !<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche, la régression linéaire revient à utiliser des fonctions d’activation linéaires pour les couches intermédiaires et une fonction d’activation linéaire ou de de type « $Relu$ » pour la couche de sortie (puisque les valeurs observées $Y$ sont $\\geq 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On montre aisément que :\n",
    "\n",
    "- pour la fonction sigmoïde : $(\\sigma(z))^{’} = 1 - \\sigma(z)$<br>\n",
    "- pour la fonction « tanh » :   $(\\tanh(z))^{’} = 1 – \\tanh(z)$<br>\n",
    "- pour la fonction « Relu » :<br>\n",
    "    $(Relu(z))^{’} = \\begin{cases} \n",
    "    0 & z < 0 \\\\\n",
    "    1 & z \\geq 0 \\\\ \n",
    "\\end{cases}$\n",
    "- pour la fonction « $Leaky Relu$ » :<br>\n",
    "    $(LeakyRelu(z))^{’} = \\begin{cases} \n",
    "    0.01 & z < 0 \\\\\n",
    "    1 & z \\geq 0 \\\\ \n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **shallow neural network** est un réseau de neurones composé d’un petit nombre de couches cachées composées d’un petit nombre de neurones.<br>\n",
    "Exemple : prenons un réseau de neurones composé de $L = 2$ couches.<br>\n",
    "Dans ce cas, le réseau possède une couche cachée.<br>\n",
    "De plus, ce réseau va posséder comme paramètres les matrices des poids et de biais :\n",
    "$W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note $n_x = n^{[0]}$ (nombre de neurones correspondant au nombre de features en entrée du réseau), $n^{[1]}$ (nombre de neurones de la couche cachée) et $n^{[2]} = 1$ (nombre de neurones de la couche de sortie) sont les nombres de neurones pour chacune des couches du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^{[1]}$ est de dimension $(n^{[1]}, n^{[0]})$ et $b^{[1]}$ est de dimension $(n^{[1]},1)$<br>\n",
    "$W^{[2]}$ est de dimension $(n^{[2]}, n^{[1]})$ et $b^{[2]}$ est de dimension $(n^{[2]},1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de coût s’écrit : $J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i = 1}^{m}L(\\hat{y},y)$<br>\n",
    "avec :\n",
    "- $\\hat{y} = a^{[2]}$\n",
    "- La fonction de coût $L$ pour une observation est identique à celle de la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la méthode « **computation graph** » qui utilise beaucoup la méthode « chain rule » dans le cas d’une seule observation,  on voit que la plupart des calculs se retrouvent facilement. <br>\n",
    "Ci-dessous, le cas de la régression logistique :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/computing_gradients_logistic_regression.png\" style=\"width:700px;height:400;\">\n",
    "<caption><center> <u> **Figure 1** </u>: **Le calcul des gradients pour la régression logistique**<br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et dans le cas d’un réseau de neurones : <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/computing_gradients_NN.png\" style=\"width:700px;height:400;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **Le calcul des gradients pour un réseau de neurones**<br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’algorithme du gradient descent va s’écrire :\n",
    "\n",
    "- Initialisation des paramètres : $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$\n",
    "- répéter <br>\n",
    "$\\hspace{10mm}${ - Calcul des prédictions $\\hat{y}^{(i)}$ pour i = 1, …,m<br>\n",
    "$\\hspace{10mm}$  - Calcul des gradients :<br>\n",
    "$\\hspace{10mm} dW^{[1]} = \\frac{\\partial J}{\\partial w_1}, db^{[1]} = \\frac{\\partial J}{\\partial b_1}$<br>\n",
    "$\\hspace{10mm} dW^{[2]} = \\frac{\\partial J}{\\partial w_2}, db^{[2]} = \\frac{\\partial J}{\\partial b_2}$<br>\n",
    "<br>\n",
    "$\\hspace{10mm}$  - Mise à jour des poids : <br>\n",
    "$\\hspace{10mm} W^{[1]} := W^{[1]} - \\alpha \\cdot dW^{[1]}$<br>\n",
    "$\\hspace{10mm} W^{[2]} := W^{[2]} - \\alpha \\cdot dW^{[2]}$<br>\n",
    "$\\hspace{10mm}$}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB)** La partie ci-dessus, entre accolades ne correspond qu’à une seule itération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut formaliser les parties « **forward propagation** » et « **back propagation** » pour $m$ exemples d’apprentissage :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation** :<br>\n",
    "<br>\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]}$<br>\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$<br>\n",
    "$Z^{[2]} = W^{[2]}A^{[1]}   + b^{[2]}$<br>  \n",
    "$A^{[2]} = g^{[2]}(Z^{[2]}) =  \\sigma(Z^{[2]})$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward propagation** :<br>\n",
    "<br>\n",
    "$dZ^{[2]} = A^{[2]} - Y \\hspace{10mm}(1) $<br>\n",
    "avec : $Y = [y(1) y(2) … y(m)]$<br>\n",
    "$dW^{[2]} = (1/m) * dZ^{[2]} A^{[1]}.T \\hspace{10mm}(2)$<br>\n",
    "$db^{[2]} = (1/m) * np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\hspace{10mm}(3)$<br>\n",
    "$dZ^{[1]} = (W^{[2]})^T dZ^{[2]} * (g^{[1]})'(Z^{[1]})\\hspace{10mm}(4)$<br>\n",
    "avec : $dA[1] = (W^{[2]})^T dZ^{[2]}$<br>\n",
    "$dA^{[1]} = (1/m)* dZ^{[1]}X.T \\hspace{10mm}(5)$<br>      \n",
    "$db^{[1]} = (1/m) * np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\hspace{10mm}(6)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque 1 :<br>\n",
    "$dZ^{[1]}$ est de dimension $(n^{[1]}, m)$<br>\n",
    "$dA^{[1]}$ est de dimension $(n^{[1]}, m)$<br>\n",
    "$(g^{[1]})'(Z^{[1]})$ est aussi de dimension $(n^{[1]}, m)$<br>\n",
    "Le produit * dans : $dZ^{[1]} = (W^{[2]})^T dZ^{[2]} * (g^{[1]})^{'}(Z^{[1]})$ est le produit élément par élement pour des matrices.<br>\n",
    "<br>\n",
    "Remarque 2 :<br>\n",
    "Une bonne compréhension des formules « back propagation » passe d’abord par la compréhension du cas d’une seule observation, ensuite on peut justifier le passage à m observations en utilisant le calcul matriciel ainsi que le fait de diviser par m notamment pour les équations :<br>\n",
    "(2), (3), (5) et (6)<br>\n",
    "<br>\n",
    "Remarque 3: <br>\n",
    "pour les équations (1) et (4), on exprime les expressions de $dZ^{[1]}$ et $dZ^{[2]}$ en généralisant le calcul de « $dz$ » d’une régression logistique car on avait :\n",
    "$\\hspace{10mm}dz = da.g'(z)$<br>\n",
    "<br>\n",
    "Remarque 4 (faite par moi) :<br>\n",
    "Les formules de « back propagation » peuvent aussi être formalisées de manière plus générale grâce au formalisme des équations fondamentales de « back propagation » proposées par Michaël Nielsen, mais en gardant plutôt un raisonnement avec une seule observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialisation aléatoire des paramètres** :<br>\n",
    "Attention, on ne peut pas initialiser tous les paramètres  $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$ à zéro comme dans pour la régression logistique. <br>\n",
    "Car si on le faisait, on obtiendrait alors des valeurs de $Z$ et $A$ exactement les mêmes quelles que soientt les couches !<br>\n",
    "\n",
    "Ce qu’il convient de faire c’est d’initialiser avec :\n",
    "\n",
    "$W^{[1]} = np.random.randn((2,2))*0.01$<br>\n",
    "$b^{[1]} = np.zeros((2,1))$<br>\n",
    "$W^{[2]} = np.random.randn((1,2))*0.01$<br>\n",
    "$b^{[2]} = np.zeros((1,1))$<br>\n",
    "\n",
    "Pour initialiser les valeurs de $b^{[1]}$ et $b^{[2]}$, on peut sans problème initialiser avec des valeurs nulles.<br>\n",
    "Par contre, pour initialiser les valeurs de $W^{[1]}$ et $W^{[2]}$, on prend plutôt une constante de 0.01 au lieu de 100,<br>\n",
    "car si on prend 100 cela veut dire que les poids vont être grands ce qui entraînerait que $Z$ va être très grand et donc les valeurs de $g'$ vont être très petites  et le « gradient descent » sera alors très lent !\n",
    "\n",
    "En fait cette valeur de 0.01 est très adaptée pour les réseaux de type « shallow » avec un petit nombre de couches cachées. Mais pour des réseaux plus profonds, on pourra choisir une constante différente de 0.01.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heroes of Deep learning** :<br>\n",
    "    **Ian Goodfellow** interview : d’abord étudiant en neuro-sciences, il s’est ensuite intéressé à l’AI. Tout est parti lorsqu’il a étudie l’article « deep belief net » de Geoff Hinton, et avec un ami il a construit une application GPU pour exécuter une machine Boltzmann pendant ses vacances d’été. Et il réalisé que le Deep learning était le domaine avec les algorithmes les plus intéressants contrairement à SVM pour lequel même avec de grandes quantités de données la performance n’était pas au rendez-vous. Richard Reyne l’a remercié pour un travail concernant un article sur la GPU. D’abord ils on utilisé leur argent pour construire leurs premières machines puis ils ont utilisé l’argent des laboratoires de Stanford. Ian est l’inventeur d’un algorithme qui a pris de l’importance en Deep learning : les GANS’s ie les Generative Adversarial Networks. On les appelle aussi les modèles génératifs, ce sont des modèles qui sont utilisables si on possède beaucoup de données d’apprentissage et si on souhaite créer plus d’exemples d’apprentissage (imaginaires). Ian Goodfellow avait  étudié les avantages et inconvénients des autres méthodes comme les machines de Boltzmann et sparse coding. Le fonctionnement de GANS’s a donné de très bons résultats dès la première tentative sans avoir à rechercher des valeurs des hyperparamètres. Actuellement, les GANS’s servent dans le domaine de l’apprentissage supervisé à générer des données d’apprentissage pour d’autres modèles et à simuler des expériences scientifiques. Il y a 10 ans, les machines de Boltzmann étaient utilisées comme des blocs importants, mais elles étaient très capricieuses. C’est pourquoi les chercheurs se sont tourné vers les fonctions Relu et la normalisation de type « batch » et c’est ce qui a rendu le Deep learning plus fiable.Si il est possible de rendre les GAN’s aussi fiables que ce qu’est devenu le Deep learning alors les GANS’s auront une place plus importante. Ian Good fellow passe maintenant plus de 40 % de son temps à essayer de stabiliser les GANS’s. De plus, Indico, Facebook et Berkeley sont aussi des pionniers des GANS’s. Il a aussi été co-auteur pour le livre sur le Deep learning avec Yoshua Bengio et Aaron Courville qui étaient ses professeurs durant son Phd. C’est un livre sur le Deep learning(70000 ventes en anglais et chinois) et de nombreux d’étudiants ont appris beaucoup grâce à ce bouquin.Dans ce livre, les maths nécessaires pour le Deep learning sont abordées. Le Machine learning a permis de résoudre des problèmes simples jusqu’à présent. Mais maintenant, il y a différentes pistes pour le Deep learning possibles et le défi est de savoir quelles orientations, le Deep learning doit prendre. Pas besoin de Phd selon lui pour travailler pour des entreprises, il faut savoir montrer son travail via github surtout si on a travaillé sur une problématique intéressante, voire un article. Si on a lu le livre, il faut aussi pratiquer par exemple sur un sujet qui nous intéresse ou sur un « street view house numbers » classifieur ou appliquer les compétences basiques du livre. Ian a aussi travaillé sur les « Adversarial examples »."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
