{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cours n°1 - semaine 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN à L couches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un réseau Deep learning à L couches, quand on compte le nombre de couches du réseau, il faut tenir compte de la couche de sortie L.\n",
    "\n",
    "Exemple: soit un réseau NN (Neural Networks) avec L = 4 couches\n",
    "On a 3 couches cachées + la couche de sortie, ce qui donne 3 + 1 = L.<br>On note $n^{[l]}$ le nombre de neurones de la couche $l$\n",
    "\n",
    "La couche d'entrée est celle d'indice 0.<br>Et on aura $n^{[0]}, n^{[1]},n^{[2]}, n^{[3]}$ et $n^{[4]} = n^{[L]} = 1$<br>\n",
    "avec : $n^{[0]} = n_x$ nombre de features de la couche d'entrée   et $n^{[4]} = n^{[L]} = 1$\n",
    "    \n",
    "Les activations de la couche $l$ sont notées $a^{[l]}$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$ avec $W^ {[l]}$ et $b^{[l]}$ qui sont les poids qui servent à calculer $z^{[l]}$<br>\n",
    "On note : $x = a^{[0]}$ et $\\hat{y} = a^{[L]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation dans un NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le cas de la Forward propagation, on va d'abord traiter du cas d'une seule observation, ensuite le cas de la vectorisation\n",
    "sera abordée avec tous les exemples d'apprentissage.<br>\n",
    "On part de la couche d'entrée, des variables ou features \"x\". Dans notre cas, nous avons 3 features x1, x2 et x3.\n",
    "\n",
    "On calcule  $z^{[1]} = W^{[1]}x + b^{[1]}$, on utilise des minuscules pour z et a car on est dans le cas d'une seule observation.\n",
    "Puis : $a^{[1]} = g^{[1]}(z^{[1]})$<br>\n",
    "\n",
    "Pour la couche 2:<br>\n",
    "$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ et $$a^{[2]} = g^{[2]}(z^{[2]})$$<br>\n",
    "Pour la couche 3:<br> \n",
    "    $$z^{[3]} = W^{[3]}a^{[2]} + b^{[3]}$$ et $$a^{[3]} = g^{[3]}(z^{[3]})$$<br>\n",
    "Pour la couche 4:<br> \n",
    "    $$z^{[4]} = W^{[4]}a^{[3]} + b^{[4]}$$ et $$a^{[4]} = g^{[4]}(z^{[4]})$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est dans le cas d'un réseau NN à 4 couches, ci-dessous le réseau:<br>\n",
    "<img src=\"images/NN_4_layers.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La règle générale pour la couche $l$, va être :<br>\n",
    "    $$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$<br>\n",
    "    $$a^{[l]} = g^{[l]}(z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation avec vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la vectorisation, cette fois on va avoir avec m exemples d'apprentissage en utilisant des majuscules pour \"z\" et \"a\":<br>\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]}$$ et $$A^{[1]} = g^{[1]}(Z^{[1]})$$  avec : $X = A^{[0]}$<br>           \n",
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$ et $$A^{[2]} = g^{[2]}(Z^{[2]})$$<br>\n",
    "\n",
    "En fait, les \"z\" pour une observation peuvent être mis côte à côte de manière à obtenir la matrice $Z^{[2]}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[2]} = \\begin{bmatrix} \n",
    "| & | & ... & | \\\\ \n",
    "z^{[2](1)} & z^{[2](2)} & ... & z^{[2](m)} \\\\ \n",
    "| & | & ... & |  \n",
    "\\end{bmatrix}$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque:<br>le nombre de lignes de cette matrice est égal à $n^{[2]}$ = nombre de neurones de la couche n°2, ...\n",
    "En continuant jusqu'à la couche n°4, on a :    \n",
    "$$Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]}$$ et $$A^{[4]} = g^{[4]}(Z^{[4]})$$<br>\n",
    "Et on note $$\\hat{Y} = g(Z^{[4]}) = a^{[4]}$$<br>\n",
    "\n",
    "Ce qui donne comme règle générale : <br>\n",
    "    $$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$<br>\n",
    "    $$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "avec : $A^{[0]} = X$<br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, la règle générale peut être appliquée pout $l=1,...,4$<br>\n",
    "Cette fois, on sera obligé d'appliquer une boucle for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il est important de bien définir les dimensions des matrices.\n",
    "Ci-dessous un réseau NN à 5 couches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NN_5_layers.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons la couche 1 pour le cas d'une seule observation :\n",
    "on a : \n",
    "    $z^{[1]} = W^{[1]}x + b^{[1]}$<br>\n",
    "    \n",
    "avec $z^{[1]}$ qui est de dimension $(3,1)$ et $x$ qui est de dimension $(2,1)$<br>\n",
    "Pour que le produit $W^{[1]}x$ soit possible, il faut alors que $W^{[1]}$ soit de dimension $(3,2)$<br>\n",
    "Autrement dit que $W^{[1]}$ soit de dimension $(n^{[1]},n^{[0]})$\n",
    "\n",
    "En raisonnant de la même façon pour la couche 2: \n",
    "sachant que $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$<br>\n",
    "On va en déduire que $W^{[2]}$ est de dimension $(5,3)$, soit de dimension $(n^{[2]},n^{[1]})$\n",
    "\n",
    "Et aussi on aura :<br>\n",
    "    $W^{[3]}$ de dimension $(n^{[3]},n^{[2]})$\n",
    "    $W^{[4]}$ de dimension $(n^{[4]},n^{[3]})$\n",
    "D'où la régle générale :<br>\n",
    "$W^{[l]}$ de dimension $(n^{[l]},n^{[l-1]})$<br>\n",
    "\n",
    "Pour ce qui concerne $b^{[l]}$, pour la couche 1 comme il faut ajouter $b^{[1]}$ à $W^{[1]}x$<br>\n",
    "$b^{[1]}$ sera de même dimension que $z^{[1]}$ donc $b^{[1]}$ sera de dimension $(3,1)$, soit $(n^{[1]},1)$<br>\n",
    "$b^{[2]}$ sera de dimension $(n^{[2]},1)$<br>\n",
    "$b^{[3]}$ sera de dimension $(n^{[3]},1)$<br><br>\n",
    "D'où la régle générale :<br>\n",
    "$b^{[l]}$ est de dimension $(n^{[l]},1)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc comme règles générales :<br>\n",
    "$W^{[l]}$ est de dimension $(n^{[l]},n^{[l-1]})$<br>\n",
    "$b^{[l]}$ est de dimension $(n^{[l]},1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec \"back propagation\", on aura $dw$ et $db$ de mêmes dimensions que $W$ et $b$, d'où:<br>\n",
    "$dW^{[l]}$ est de dimension $(n^{[l]},n^{[l-1]})$<br>\n",
    "$db^{[l]}$ est de dimension $(n^{[l]},1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, comme : $a^{[l]} = g^{[l]}(z^{[l]})$<br>\n",
    "    $a^{[l]}$ et $z^{[l]}$ sont de mêmes dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec la vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la \"vectorisation\" ie avec plusieurs exemples d'apprentissage, les dimensions de $W^{[l]}$, $b^{[l]}$, $dW^{[l]}$ et $db^{[l]}$ vont rester les mêmes. Mais les dimensions de $Z^{[l]}$ et $A^{[l]}$ vont changer par rapport $z^{[l]}$ et $a^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On avait pour une observation, pour la couche 1:<br>\n",
    "    $$z^{[1]} = W^{[1]}x + b^{[1]}$$\n",
    "avec :<br>\n",
    "     $z^{[1]}$ de dimension $(n^{[1]},1)$, $W^{[1]}$ de dimension $(n^{[1]},n^{[0]})$ de dimension, $x$ de dimension $(n^{[0]},1)$ et $b^{[1]}$ de dimension $(n^{[1]},1)$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on  m observations, on l'écriture matricielle :<br>\n",
    "    $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "    avec $Z^{[1]}$ qui s'écrit :<br><br>\n",
    "    $Z^{[1]} = \\begin{bmatrix} \n",
    "| & | & ... & | \\\\ \n",
    "z^{[1](1)} & z^{[1](2)} & ... & z^{[1](m)} \\\\ \n",
    "| & | & ... & |  \n",
    "\\end{bmatrix}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois les dimensions sont:<br> \n",
    "$(n^{[1]},n^{[0]})$ pour $W^{[1]}$,\n",
    "$(n^{[0]},m)$ pour $X$,\n",
    "$(n^{[1]},1)$ pour $b^{[1]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le produit $W^{[1]}X$ est alors de dimension $(n^{[1]},m)$<br>\n",
    "Et avec la technique du \"broadcasting\" de Python, $b^{[1]}$ va être dupliqué de manière à avoir $b^{[1]}$ sous la forme d'une matrice de dimension $(n^{[1]},m)$\n",
    "puis ajoutée élément par élement à $W^{[1]}X$<br>\n",
    "\n",
    "On vait $z^{[l]}$ et $a^{[l]}$ de dimensions $(n^{[l]},1)$.<br>\n",
    "Maintenant, $Z^{[l]}$ et $A^{[l]}$ sont de dimensions $(n^{[1]},m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons le cas particulier $l =0$ avec $A^{[0]} = X$ de dimension $(n^{[0]},m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus : $dZ^{[l]}$ et $dA^{[l]}$ sont aussi de dimensions $(n^{[1]},m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On peut aussi se poser la question qu'entend on par réseaux de neurones \"profonds\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deep_features_images.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pourquoi faut-il que les NN possèdent beaucoup de couches cachées ?<br>La 1ère couche va permettre de détecter des features ou des \"edges\"<br> \n",
    "Par exemple si on fait de l'apprentissage d'images, une image sur la 1ère couche peut se décomposer en neurone représentant un feature ou \"edge\" ayant une orientation verticale\n",
    "et en neurone représentant un feature ou \"edge\" ayant une orientation horizontale.Sachant qu'on pourrait avoir par exemple 20 neurones cachés pour la 1ère couche cachée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur la couche suivante, le NN peut alors regrouper et utiliser les \"edges\" pour former des parties d'un visages grâce aux différents neurones de la 2ème couche<br>\n",
    "Et enfin, le NN va construire via les neurones finaux les différents types de visage !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On verra cela plus concrètement avec les Convolutional NN<br>\n",
    "Autre exemple que les images:<br>\n",
    "    Pour l'audio, la 1ère couche va détecter des \"low level audio form features\", cad est-ce que la musique ou le son \"monte\" ou \"descend\"? Ou y a t -il du bruit blanc ?\n",
    "    Ou du \"slithering sound\" ou du \"pitch\" ?<br>\n",
    "Ensuite, le NN va pouvoir détecter des unités basiques de sons, cad des phonèmes (sons \"C\" ou \"A\" ou \"T\")<br>\n",
    "Afin de reconnaître des mots dans l'enregistrement audio, puis reconnaître des phrases entières !<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but va vraiment être de détecter des simples features, puis des features de plus en plus complexes grâces aux couches suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut citer le concept de \"circuit theory\" pour le Deep learning. En effet, utiliser plusieurs couches composées d'un très petit nombre de neurones est plus efficace\n",
    "que d'utiliser des \"shallow NN\" (avec un nombre de couches petit mais un grand nombre de neurones)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer des fonctions \"XOR\" imbriqués à n features va être équivalent à un NN à une couche avec un très grand nombre de neurones de l'ordre de $2^{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/XOR_vs_shallow_NN.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau de fonctions \"XOR\" imbriquées va avoir une complexité de calcul algorithmique de l'ordre $O(log(n))$ alors que celle du NN à une couche et $2^{n-1}$ neurones\n",
    "sera de $O(2^{n})$<br>\n",
    "Il faut savoir aussi qu'une fonction \"XOR\" est équivalente à une couche cachée avec certains poids \"w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de blocs de NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NN_couche_l.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sait que pour chaque couche $l$:<br>\n",
    "    Pour \"forward propagation\", pour une observation :<br>\n",
    "    $$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$<br>\n",
    "    $$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "    \n",
    "$z^{[l]}$ va être mis en cache car la connaissance des valeurs de $z^{[l]}$ va servir pour l'étape \"backward propagation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour \"backward propagation\", on a :<br>\n",
    "    la matrice $da^{[l]}$ est connue en entrée ainsi qu'en \"cache\", $z^{[l]}$ est aussi connue, et en sortie on va calculer :<br>\n",
    "    $da^{[l-1]}$ ainsi que les matrices $dW^{[l]}$ et $db^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi ces 2 étapes respectives \"forward prop\" et \"backward prop\", sont 2 étapes importantes à implémenter en raisonnant pour une couche $l$ générique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fun_to_implement_forward_backward_couche_l.jpg\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N'oublions pas que la finalité est de calculer pour chaque itération :<br>\n",
    "toutes les matrices $dW^{[1]}$, $dW^{[2]}$, $dW^{[3]}$, ..., $dW^{[L]}$ \n",
    "                    $db^{[1]}$, $db^{[2]}$, $db^{[3]}$, ..., $db^{[L]}$\n",
    "    \n",
    "Et de calculer en sortie simultanément:<br>\n",
    "    $$W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L]}$$\n",
    "    grâce aux formules:<br>\n",
    "        $$W^{[l]} :=  W^{[l]} - \\alpha dW^{[l]}$$\n",
    "        $$b^{[l]} :=  b^{[l]} - \\alpha db^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation pour la couche $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour une observation :\n",
    "En entrée on a : $a^{[l-1]}$\n",
    "En sortie on obtient $a^{[l]}$ et aussi $z^{[l]}$ qui sera mis en \"cache\" suivant :<br>\n",
    "    $$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$<br>\n",
    "    $$a^{[l]} = g^{[l]}(z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec vectorisation avec $m$ observations:\n",
    " $A^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$<br>\n",
    " $A^{[l]} = g^{[l]} (Z^{[l]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagation pour la couche $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pour une observation :\n",
    "En entrée, on a : $da^{[l]}$<br>\n",
    "En sortie, on a : $da^{[l-1]}$, $dW^{[l]}$ et $db^{[l]}$<br>\n",
    "On va avoir les 4 formules ci-dessous qui permettent d'aller en arrière ci-dessous !<br>\n",
    "En calculant d'abord $dz^{[l]}$: <br>\n",
    "$$dz^{[l]} =  da^{[l]}*g^{[l]^{'}}(z^{[l]})$$ où * désigne le produit élément par élément pour les matrices <br>\n",
    "$$dW^{[l]} =  dz^{[l]}a^{[l-1]}$$ <br> où $dz^{[l]}a^{[l-1]}$ est un produit de matrices<br>                       \n",
    "$$db^{[l]} =  dz^{[l]}$$\n",
    "$$da^{[l-1]} = (W^{[l]})^\\top dz^{[l]}$$                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : <br>$dz^{[l]}$ s'écrit en fait:<br>\n",
    "          $$dz^{[l]} = (W^{[l+1]})^\\top dz^{[l+1]} * g^{[l]^{'}}(z^{[l]})$$<br>\n",
    "avec : $$da^{[l]} = (W^{[l+1]})^\\top dz^{[l+1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avec vectorisation avec  $m$  observations, ci-dessous les 4 formules avec formalisme matriciel :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dZ^{[l]} =  dA^{[l]}*g^{[l]^{'}}(z^{[l]})$$ où * désigne le produit élément par élément pour les matrices <br>\n",
    "$$dW^{[l]} =  \\frac{1}{m} dZ^{[l]}(A^{[l-1])^\\top} $$ avec $dZ^{[l]}(A^{[l-1])^\\top}$ qui est un produit de matrices<br>                    \n",
    "$$db^{[l]} =  \\frac{1}{m} np.sum (dZ^{[l]}, axis = 1, keepdims = True )$$<br>\n",
    "$$dA^{[l-1]} = (W^{[l]})^\\top dZ^{[l]}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que pour calculer toute l'étape \"back prop\", il faut avoir mis en cache lors de l'étape \"forward prop\" : <br> \n",
    "    $$z^{[l]}, a^{[l-1]}, W^{[l]}, b^{[l]}$$ lorsque on travaille avec une seule observation\n",
    "Et :<br>\n",
    "    $$Z^{[l]}, A^{[l-1]}, W^{[l]}, b^{[l]}$$ lorsque on fait de la vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autre point important : l'initialisation de \"back prop\" qui se réalise en calculant, pour la couche $L$, le terme $da^{[l]}$ ou $dA^{[l]}$ pour $m$ observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On montre grâce à l'expression de la fonction de coût pour une observation : $L(\\hat{y},y) = -y log(a) - (1 - y) log(1 - a)$:<br>\n",
    "que : $$da^{[L]} = - \\frac{y}{a} + \\frac{1-y}{1-a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui donne pour $m$ observations:<br>\n",
    "    \n",
    "$$dA^{[L]} = \\begin{bmatrix}  \n",
    "- \\frac{y^{[1]}}{a^{[1]}} + \\frac{1-y^{[1]}}{1-a^{[1]}} & - \\frac{y^{[2]}}{a^{[2]}} + \\frac{1-y^{[2]}}{1-a^{[2]}} & ... & - \\frac{y^{[m]}}{a^{[m]}} + \\frac{1-y^{[m]}}{1-a^{[m]}} \\\\  \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramètres et hyperparamètres\n",
    "Par la suite, il faudra bien distinguer les paramètres :<br>\n",
    "$W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$, ..., $W^{[L]}$, $b^{[L]}$<br>\n",
    "des hyperparamètres:<br>\n",
    "learning rate $\\alpha$<br>\n",
    "le nombre d'itérations<br>\n",
    "le nombre de couches cachées L<br>\n",
    "le nombre de neurones par couche $n^{[1]}$, $n^{[2]}$, $n^{[3]}$, ..., $n^{[L]}$<br>\n",
    "le choix des fonctions d'activation : Relu, tanh, sigmoid, softmax, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus tard on verra d'autres hyperparamètres utilisés en Deep learning comme:\n",
    "Momentum, le \"minibatch size\" ainsi que différentes formes de régularisation, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, le DL est basé sur un process expérimental. Il est difficile de connaître à l'avance les hyperparamètres<br>\n",
    "Il faut essayer différentes valeurs d'hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut aussi observer comment décroît la fonction de coût J en fonctions du nombre d'itérations \n",
    "et s'assurer de la décroissance de J en fonction du nombre d'itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les applications en DL sont les suivantes :<br>\n",
    "Computer Vision<br>\n",
    "Speech recognition<br>\n",
    "NLP<br>\n",
    "Advertising  application sur données structurées<br>\n",
    "Web search  application sur données structurées<br>\n",
    "Recommendations de produits application sur données structurées<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention : même pour un problème sur lequel on travaille depuis longtemps, un hyperparamètre peut être à changer.\n",
    "Car un paramétrage technique peut amener à devoir changer un hyperparamètre afin que l'algorithme d'apprentissage reste performant.\n",
    "Par exemple, le nombre de CPU ou de GPU peut amener une modification d'un hyperparamètre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A propos de l'analogie avec le cerveau :\n",
    "Il y a vraiment une analogie entre une régression logistique représentée avec un neurone et une fonction sigmoïde et le fonctionnement d'un neurone du cerveau.<br>\n",
    "Néanmoins, le fonctionnement des neurones au sein du cerveau reste très mystérieux. Et on ne peut pas l'expliquer simplement par un algorithme forward et backward propagation.Son fonctionnment est beacoup\n",
    "plus complexe. L'analogie avec le neurone humain a été faite au départ pour introduire les NN, mais on ne peut pas vraiment continuer à la faire avec les Deep NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1484px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
